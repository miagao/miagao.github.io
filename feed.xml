<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Miagao</title>
    <description>Awesome blog about Software Development, Operations Research, Education and Behavioral Finance.
</description>
    <link>http://imiagao.github.io/</link>
    <atom:link href="http://imiagao.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 05 Aug 2015 16:07:50 -0300</pubDate>
    <lastBuildDate>Wed, 05 Aug 2015 16:07:50 -0300</lastBuildDate>
    <generator>Jekyll v2.5.3</generator>
    
      <item>
        <title>Welcome to Jekyll!</title>
        <description>&lt;p&gt;You’ll find this post in your &lt;code&gt;_posts&lt;/code&gt; directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run &lt;code&gt;jekyll serve&lt;/code&gt;, which launches a web server and auto-regenerates your site when a file is updated.&lt;/p&gt;

&lt;p&gt;To add new posts, simply add a file in the &lt;code&gt;_posts&lt;/code&gt; directory that follows the convention &lt;code&gt;YYYY-MM-DD-name-of-post.ext&lt;/code&gt; and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.&lt;/p&gt;

&lt;p&gt;Jekyll also offers powerful support for code snippets:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Hi, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Tom&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#=&amp;gt; prints &amp;#39;Hi, Tom&amp;#39; to STDOUT.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Check out the &lt;a href=&quot;http://jekyllrb.com&quot;&gt;Jekyll docs&lt;/a&gt; for more info on how to get the most out of Jekyll. File all bugs/feature requests at &lt;a href=&quot;https://github.com/jekyll/jekyll&quot;&gt;Jekyll’s GitHub repo&lt;/a&gt;. If you have questions, you can ask them on &lt;a href=&quot;https://github.com/jekyll/jekyll-help&quot;&gt;Jekyll’s dedicated Help repository&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Wed, 05 Aug 2015 12:50:46 -0300</pubDate>
        <link>http://imiagao.github.io/jekyll/update/2015/08/05/welcome-to-jekyll.html</link>
        <guid isPermaLink="true">http://imiagao.github.io/jekyll/update/2015/08/05/welcome-to-jekyll.html</guid>
        
        
        <category>jekyll</category>
        
        <category>update</category>
        
      </item>
    
      <item>
        <title>truncate large collections based on the creation time</title>
        <description>&lt;br /&gt;Sometimes you need to truncate a large collection on MongoDB ( mine is 1.6 billion documents ) on an unindexed field, such as a Mongoid field called created_at. ( OK, TTL collections exists to solve this, but this is a transient solution before we can reindex those 1.6 B documents).&lt;br /&gt;&lt;br /&gt;This command: db.collection.remove({created_at : {$lt : ISODate(&quot;2012-10-09&quot;) }}) will trigger a full table scan which can take hours to run before any actual document is deleted. .&lt;br /&gt;&lt;br /&gt;Object Id to the rescue!!!&lt;br /&gt;&lt;br /&gt;MongoDB´s ObjectID comes with timestamp for the Object´s creation time inside for free (http://docs.mongodb.org/manual/core/object-id/)&lt;br /&gt;&lt;br /&gt;So, with a little help from stackoverflow and &amp;nbsp;and Kristina Chodorow (http://www.kchodorow.com/blog/2011/12/20/querying-for-timestamps-using-objectids/) , I created a fake ObjectID from a specified date and then removed over the collection based on the _id field, which is indexed by default, so I remove every _id created before this date!&lt;br /&gt;&lt;br /&gt;&amp;gt; time = new ISODate(&quot;2012-12-09&quot;)&lt;br /&gt;&amp;gt; ms = time.getTime()&lt;br /&gt;&amp;gt; sec = Math.floor(ms/1000)&lt;br /&gt;&amp;gt; hex = sec.toString(16)&lt;br /&gt;&amp;gt; id_string = hex + &quot;0000000000000000&quot;&lt;br /&gt;&amp;gt; my_id = ObjectId(id_string)&lt;br /&gt;&amp;gt;&lt;br /&gt;&amp;gt; db.my_collection.remove( { _id: { $lt: my_id } } )&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;A cool and indexed query, much faster than the other option!&lt;br /&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;</description>
        <pubDate>Wed, 23 Jan 2013 19:40:00 -0200</pubDate>
        <link>http://imiagao.github.io/2013/01/23/truncate-large-collections-based-on.html</link>
        <guid isPermaLink="true">http://imiagao.github.io/2013/01/23/truncate-large-collections-based-on.html</guid>
        
        
      </item>
    
      <item>
        <title>Monitor Anything on ServerDensity</title>
        <description>What´s your favorite monitoring tool? rrdtool? Nagios? &amp;nbsp;MRTG? Zabbix? Ganglia?&lt;br /&gt;&lt;br /&gt;Well, everybody has a favorite. I don´t. They are all complicated to maintain &amp;nbsp;and/or awkward to scale.&lt;br /&gt;&lt;br /&gt;For monitoring I use ServerDensity and I monitor anything I want using it´s plugin system.&lt;br /&gt;&lt;br /&gt;It took me a while to make it work at first, but just by editing a simple Python script I was able to monitor things like our &lt;b&gt;gros revenue&lt;/b&gt; in the last ten minutes and our queue size on Sidekiq which runs on a third-party Redis SaaS provider.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;Basically the idea behind its plugins is that anything that outputs a Python data struct can be graphed at serverdensity:&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;class Plugin1 (object):&lt;br /&gt;&lt;span style=&quot;line-height: 19px;&quot;&gt;&lt;span class=&quot;Apple-tab-span&quot;&gt;&lt;/span&gt;    def __init__(self, agentConfig, checksLogger, rawConfig):&lt;/span&gt;&lt;span style=&quot;line-height: 19px;&quot;&gt;&lt;span class=&quot;Apple-tab-span&quot;&gt;&lt;/span&gt;    self.agentConfig = agentConfig&lt;/span&gt;&lt;span style=&quot;line-height: 19px;&quot;&gt;&lt;span class=&quot;Apple-tab-span&quot;&gt;&lt;/span&gt;    self.checksLogger = checksLogger&lt;/span&gt;&lt;span style=&quot;line-height: 19px;&quot;&gt;&lt;span class=&quot;Apple-tab-span&quot;&gt;&lt;/span&gt;    self.rawConfig = rawConfig&lt;/span&gt;&lt;span class=&quot;Apple-tab-span&quot; style=&quot;line-height: 19px;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;span style=&quot;line-height: 19px;&quot;&gt;&lt;span class=&quot;Apple-tab-span&quot;&gt;&lt;/span&gt;    def run(self):&lt;/span&gt;&lt;span style=&quot;line-height: 19px;&quot;&gt;&lt;b&gt;&lt;span class=&quot;Apple-tab-span&quot;&gt;&lt;/span&gt;&lt;span style=&quot;color: red;&quot;&gt;        data = {&#39;hats&#39;: 5, &#39;Dinosaur Rex&#39;: 25.4}&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;span style=&quot;line-height: 19px;&quot;&gt;&lt;span class=&quot;Apple-tab-span&quot;&gt;&lt;/span&gt;        return data&lt;/span&gt;&lt;/pre&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;Read the whole story here:&lt;br /&gt;http://support.serverdensity.com/knowledgebase/articles/76018-writing-a-plugin-linux-mac-and-freebsd&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;A little fiddling and I can call MongoDB using javascript, I can call URL´s or a redis-client program to read the queue status on Sidekiq, and so many things.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;And that´s about it, the data will start to be collected and graphed for me instantly. That´s great for capacity planning and application debugging. &lt;br /&gt;&lt;br /&gt;A little more tweaking and we can also send Alerts to yourself or to PagerDuty based on custom thresholds on those features you just added on ytour custom plugin: for instance, you can be paged when your &lt;b&gt;&lt;span style=&quot;color: red;&quot;&gt;&#39;hats&#39;&lt;/span&gt;&lt;/b&gt; reach a limit of 10 or your&lt;b&gt;&lt;span style=&quot;color: red;&quot;&gt; &#39;Dinossaur Rex&#39; &lt;/span&gt;&lt;/b&gt;reaches 100.0&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;Another good use of ServerDensity is it´s cool blog and it´s weekly tips: the &quot;Sysadmin Sunday&quot;.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;h2 class=&quot;entry-title&quot; style=&quot;background-color: white; color: #1155cc; font-family: arial, sans-serif; font-size: 18px; margin: 0px; max-width: 650px;&quot;&gt;&lt;br /&gt;&lt;/h2&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;</description>
        <pubDate>Fri, 14 Dec 2012 16:18:00 -0200</pubDate>
        <link>http://imiagao.github.io/2012/12/14/monitor-anything-on-serverdensity.html</link>
        <guid isPermaLink="true">http://imiagao.github.io/2012/12/14/monitor-anything-on-serverdensity.html</guid>
        
        
      </item>
    
      <item>
        <title>Separate large collections in different Replicas ! </title>
        <description>MongoDB is great if you know how to use it. But our Production Database is reaching several limits.&lt;br /&gt;&lt;br /&gt;We are heavily using SSD backed EC2 instances. And our volume is skyrocketing. Can we shard it? Yes, but it would complicate our admin and our Rails app was not ready yet. it was not conceived &amp;nbsp;ahead for the correct shard key. Moreover, the growth we are experiencing is overwhelming.&lt;br /&gt;&lt;br /&gt;We already tried that once, splitting small collections which were very read intensive, but back then, it was only a couple of GB and the entire dump/restore lasted only a couple of minutes.&lt;br /&gt;&lt;br /&gt;Now we have two large collections to separate: &lt;b&gt;foo&lt;/b&gt;, has approximately 800 MM docs and our system is highly dependant on it. The other is &lt;b&gt;bar&lt;/b&gt;, which has 200 MM docs and we can afford a small downtime on it.&lt;br /&gt;&lt;br /&gt;So, how can we do it? Let´s separate them into two different, independent ReplicaSets!&lt;br /&gt;&lt;br /&gt;Our initial setup is a Replicaset - RS1 with 3 servers on 3 different Availability Zones containing the 2 large collections. Our intended final setup is 2 Replicasets (RS1 and RS2 ) each one with one of the large collections.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;So let´s get dirty:&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;br /&gt;1 - Sync 3 new servers&lt;br /&gt;2 - Stop all 3 new servers&lt;br /&gt;&lt;b&gt;3- Remove them from RS1, fail to do that and the application will still try to read them.&amp;nbsp;&lt;/b&gt;&lt;br /&gt;4 - Start up each new server without the --replSet option, pointing at the correct data directory.&lt;br /&gt;5 - Update the local.system.replset doc on each server with the new replica set name. You have to change every server here.&lt;br /&gt;&lt;br /&gt;Here´s a little snippet showing this part, and a reference for the credits (http://stackoverflow.com/questions/11265997/can-i-change-the-name-of-my-replica-set-while-mongod-processes-are-running):&lt;br /&gt;&lt;br /&gt;use local&lt;br /&gt;cfg = db.system.replset.findOne()&lt;br /&gt;cfg._id = &quot;RS2&quot;&lt;br /&gt;&lt;br /&gt;cfg.members[4].votes=1&lt;br /&gt;&lt;div&gt;cfg.members[5].votes=1&lt;/div&gt;&lt;div&gt;cfg.members[6].votes=1&lt;/div&gt;&lt;br /&gt;cfg.members = [cfg.members[4] , cfg.members[5] , cfg.members[6]]&lt;br /&gt;db.system.replset.update({}, cfg)&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;6 - Shut them down again.&lt;br /&gt;7 - Change the /etc/mongod.conf replSet option to the new name &quot;RS2&quot; on the new servers.&lt;br /&gt;8 - Start them all up with the original options.&lt;br /&gt;9 - Check that it´s reading and writing from the new RS .&lt;br /&gt;10 - Remove collection &lt;b&gt;foo&lt;/b&gt; from RS1, remove &lt;b&gt;bar&lt;/b&gt; from RS2.&lt;br /&gt;11 - ( optional ) Rest for a month or so until you grow another 200%. Again.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;Caveats:&lt;br /&gt;- Make the new servers non-voters when you create them on the old RS and voters on the newer.&lt;br /&gt;- Make tests on dummies before apply it on production.&lt;br /&gt;- Rename the less critical one, &amp;nbsp;because it requires downtime, but if you REALLY need the information updated on the second one during the downtime, there are some opensource tools in Java (&amp;nbsp;&lt;span style=&quot;background-color: white; color: #222222; font-family: arial, sans-serif; font-size: x-small; line-height: 16px;&quot;&gt;https://github.&lt;/span&gt;&lt;wbr style=&quot;background-color: white; color: #222222; font-family: arial, sans-serif; font-size: small; line-height: 16px;&quot;&gt;&lt;/wbr&gt;&lt;span style=&quot;background-color: white; color: #222222; font-family: arial, sans-serif; font-size: x-small; line-height: 16px;&quot;&gt;com/wordnik/wordnik-oss)&lt;/span&gt;&amp;nbsp;or Python (&amp;nbsp;http://pypi.python.org/pypi/OplogReplay/0.1.4 ) which can do the trick with a little hack to read the oplog just on the right collection and replay it on the second replica.&lt;br /&gt;&lt;br /&gt;UPDATE:&lt;br /&gt;&lt;br /&gt;We did it that last week. It didn´t work quite as well, because when two of the databases went up, the third, which was a bit ahead of the others when stopped, refused to sync and went in a permanent FATAL state.&lt;br /&gt;Long story short, we successfully made a full resync on it and now we have two databases instead of one and, as expected, the %lock on them is approximately half of the original value on peak hours.&lt;br /&gt;&lt;br /&gt;The pressure went down on the whole system and we now have a lot less exceptions and a the responsiveness is very good.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;NEXT:&lt;br /&gt;&lt;br /&gt;Cassandra will save the day replacing our back end for analytics and aggregation. &amp;nbsp;Our preliminary tests are very promising and we hope to make it into production next week.&lt;br /&gt;&lt;br /&gt;</description>
        <pubDate>Tue, 27 Nov 2012 16:08:00 -0200</pubDate>
        <link>http://imiagao.github.io/2012/11/27/separate-large-collections-in-different.html</link>
        <guid isPermaLink="true">http://imiagao.github.io/2012/11/27/separate-large-collections-in-different.html</guid>
        
        
      </item>
    
      <item>
        <title>How to calculate the number of Heroku web Dynos</title>
        <description>Last week I had a class on Introduction to &amp;nbsp;Operations Management at Cousera.org (&amp;nbsp;https://class.coursera.org/course/operations ). &amp;nbsp;Being an engineer with operations research background, that would sound silly to hope that something there would help me being a better devops, but when I least expected, there it was sound and clear, the waiting models.&lt;br /&gt;&lt;br /&gt;I vaguely remembered about Markov chains, M/M/1 queues, that once in my life &amp;nbsp;I used to justify a SLA for a quote.&lt;br /&gt;&lt;br /&gt;The problem with the graphs at NewRelic is that it´s not clear how to calculate and justify the number of dynos on a Heroku app using those figures.&lt;br /&gt;&lt;br /&gt;It turns out it´s simple to calculate the number of dynos simply by establishing a baseline to the maximum time you can afford to make your requests wait!&lt;br /&gt;&lt;br /&gt;First and foremost the number of workers should be the minimum that attends this simple rule:&lt;br /&gt;&lt;br /&gt;u = &amp;nbsp;FLOW RATE / CAPACITY &amp;lt; 100%&lt;br /&gt;&lt;br /&gt;where FLOW RATE can be inferred in a NewRelic by requests per minute&lt;br /&gt;and CAPACITY is the number of workers ( w ) multiplied by the average time to serve a request (p - also inferred in NewRelic )&lt;br /&gt;&lt;br /&gt;u &amp;nbsp;= target RPM / ( w * avg response time)&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;( Pure common sense but many times I was getting it wrong, having just newrelic´s Apdex and &amp;nbsp;response times as parameter for setting the number of workers ) .&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;Well, that being said, let´s minimize the Time on queue Tq with the formula below:&lt;br /&gt;&lt;br /&gt;Tq = p/m * u / (1-u) * (CVa2 + CVp2)/2&lt;br /&gt;&lt;br /&gt;Lets look at the first part ( p/m ), It´s just the process time divided by the number of workers, which divide the load.&lt;br /&gt;&lt;br /&gt;The second parameter tends to infinit with values of utilization close to 100%, which sounds intuitive.&lt;br /&gt;&lt;br /&gt;And the third parameter is the normalized variance of the process time and the interval between arrivals ( 1/throughput ). For the sake of simplicity, lets assume this parcel is equal to 1, but you can get approximately this figure on NewRelic or even calculate like I did by ETL´ ing the webserver access log.&lt;br /&gt;&lt;br /&gt;Put this formula on Excel and what you get is the Time on queue depending on the number of workers you put on the formula.&lt;br /&gt;&lt;br /&gt;I found this very enlightening and a very rational way to trigger more dynos at my application and not being blamed for being too conservative.&lt;br /&gt;&lt;br /&gt;Maybe this can turn into an autoscaler gem sometime soon.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;</description>
        <pubDate>Wed, 31 Oct 2012 22:05:00 -0200</pubDate>
        <link>http://imiagao.github.io/2012/10/31/how-to-calculate-number-of-heroku-web.html</link>
        <guid isPermaLink="true">http://imiagao.github.io/2012/10/31/how-to-calculate-number-of-heroku-web.html</guid>
        
        
      </item>
    
      <item>
        <title>Opscode Chef -- howto make chef-client work</title>
        <description>After downloading and install of the chef and cloning and configuration of chef-repo and knife, I got stuck in how to install the client side (chef-client). Here´s a very brief howto.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;To install chef-client on the client&lt;br /&gt;&lt;br /&gt;sudo true &amp;amp;&amp;amp; curl -L http://opscode.com/chef/install.sh | sudo bash&lt;br /&gt;sudo mkdir /etc/chef&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;Copy config generated at the workstation:&lt;br /&gt;&lt;br /&gt;scp -pr client-config/* USERNAME@HOSTNAME:/etc/chef/&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;To run chef client:&lt;br /&gt;&lt;br /&gt;sudo chef-client -N NODENAME&lt;br /&gt;&lt;br /&gt;OR add the following line at /etc/chef/client.rb&lt;br /&gt;&lt;br /&gt;node_name NODENAMEHERE&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;</description>
        <pubDate>Thu, 09 Aug 2012 19:16:00 -0300</pubDate>
        <link>http://imiagao.github.io/2012/08/09/opscode-chef-howto-make-chef-client-work.html</link>
        <guid isPermaLink="true">http://imiagao.github.io/2012/08/09/opscode-chef-howto-make-chef-client-work.html</guid>
        
        
      </item>
    
      <item>
        <title>MongoDB EC2 SSD x IOP provisioned x EBS optimized instance comparison</title>
        <description>We are using 3 types of storage for our MongoDB stack.&lt;br /&gt;&lt;br /&gt;As our old plain 8 EBS Volumes RAID10 array were getting too busy - with %lock close to 60% which is the maximum acceptable in our experience. We&amp;nbsp; put a couple of&amp;nbsp; &lt;span class=&quot;st&quot;&gt;High &lt;em&gt;I/O SSD&lt;/em&gt;-based EC2 instances ( hi.4xlarge ). And they were very fast but they were not perfect, that´s because their storage is &lt;i&gt;instance&lt;/i&gt; based so they lack durability and we were not able to snapshot them.&amp;nbsp; But a few days ago we heard about the new &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&lt;em&gt;EBS&lt;/em&gt;-&lt;em&gt;Optimized&lt;/em&gt; instances and the &lt;i&gt;Provisioned IOP Volumes. &lt;/i&gt;It sounds confusing and it is:&amp;nbsp;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;span class=&quot;st&quot;&gt;In my view, the EBS-Optimized instances adds a new network interface giving you an extra 500 Mbps of bandwidth just to talk to EBS volumes. And the Provisioned IOP Volumes are like regular EBS, but they sell you with IOP rather than just space.&amp;nbsp;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;span class=&quot;st&quot;&gt;To get an EBS-Optimized instance, you have to choose this option at the console. Or you have to stop the instance and then change an attribute on them at the command line:&amp;nbsp;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;span class=&quot;st&quot;&gt;&amp;nbsp;#ec2-modify-instance-attribute&lt;i&gt; instance_id &lt;/i&gt;--ebs-optimized &lt;i&gt;true &lt;/i&gt;&lt;/span&gt;&lt;br /&gt;&lt;span class=&quot;st&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;BEWARE: that operation will change the DNS on that instance. So expect to have a downtime on your replicaset and don´t forget to change either your Replicaset configuration OR your DNS record to reflect this change.&lt;/span&gt;&lt;br /&gt;&lt;span class=&quot;st&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&lt;/span&gt;&lt;br /&gt;&lt;span class=&quot;st&quot;&gt;To use the Provisioned IOP Volumes, you have to choose how much IOP you want provisioned in each new EBS volume respecting the rule of having at least 1/10 of the&amp;nbsp; provisioned IOP in GB of data in each volume. If you want 1000 IOPs ( which is the maximum ), you need at least 100 GB of volume. &lt;/span&gt;&lt;br /&gt;&lt;span class=&quot;st&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&lt;br /&gt;&lt;/span&gt;The test was to see how they compare at the same role, so they all are Secondary nodes of the same Replica Set on Production, at 700 updates/sec and 100 inserts/sec on average during the tests.&lt;br /&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;br /&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;br /&gt;&lt;span class=&quot;st&quot;&gt;The setup was:&amp;nbsp;&lt;/span&gt;&lt;br /&gt;&lt;span class=&quot;st&quot;&gt;On the plain EBS -- with ebs-optimized=true (&amp;nbsp; RS1-DB01 )&lt;/span&gt;&lt;br /&gt;&lt;span class=&quot;st&quot;&gt;instance type: m2.4xlarge&amp;nbsp; &lt;/span&gt;&lt;br /&gt;&lt;span class=&quot;st&quot;&gt;8 volumes &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;mounted on /var/lib/mongodb&lt;/span&gt;&lt;br /&gt;&lt;span class=&quot;st&quot;&gt;( journaling turned off ) &lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;span class=&quot;st&quot;&gt;on the IOP provisioned EBS ( &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;RS1-IOP01 )&lt;/span&gt;&lt;br /&gt;&lt;span class=&quot;st&quot;&gt;instance type: m2.4xlarge&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;br /&gt;&lt;span class=&quot;st&quot;&gt;8 volumes with 1000 iop each&amp;nbsp; in RAID10 &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;mounted on /var/lib/mongodb &lt;/span&gt;&lt;br /&gt;&lt;span class=&quot;st&quot;&gt;1 volume with 1000 iop &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;mounted on /var/lib/mongodb/journal &lt;/span&gt;&lt;br /&gt;&lt;span class=&quot;st&quot;&gt;&lt;/span&gt;&lt;br /&gt;&lt;span class=&quot;st&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;On the SSD-based instance ( RS1-SSD02 )&lt;/span&gt;&lt;br /&gt;&lt;span class=&quot;st&quot;&gt;instance type: hi1.4xlarge&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;br /&gt;&lt;span class=&quot;st&quot;&gt;1 Volume mounted on /var/lib/mongodb&lt;/span&gt;&lt;br /&gt;&lt;span class=&quot;st&quot;&gt;1 Volume mounted on /var/lib/monogdb/journal &lt;/span&gt;&lt;br /&gt;&lt;span class=&quot;st&quot;&gt;&lt;i&gt;&lt;br /&gt;&lt;/i&gt;&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;http://4.bp.blogspot.com/-TGAzk5JqaC0/UCKjvhGfAJI/AAAAAAAAApk/AsiZ2amQaXA/s1600/Screen+Shot+2012-08-08+at+1.54.49+PM.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://4.bp.blogspot.com/-TGAzk5JqaC0/UCKjvhGfAJI/AAAAAAAAApk/AsiZ2amQaXA/s1600/Screen+Shot+2012-08-08+at+1.54.49+PM.png&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;span class=&quot;st&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&lt;br /&gt;&lt;/span&gt;These pictures show how they perform and why we will still continue to use SSD backed instances for now but will keep on IOP provisioned for snapshots. &lt;br /&gt;&lt;br /&gt;It is possible that this performance difference happens because of network latency -&amp;nbsp; the hi.4xlarge have 10Gb network interfaces instead of 1Gb. But our next steps are putting those collections in shards and we should be fine with these until that happens in a week or two ... &lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;</description>
        <pubDate>Wed, 08 Aug 2012 14:59:00 -0300</pubDate>
        <link>http://imiagao.github.io/2012/08/08/mongodb-ec2-ssd-x-iop-provisioned-x-ebs.html</link>
        <guid isPermaLink="true">http://imiagao.github.io/2012/08/08/mongodb-ec2-ssd-x-iop-provisioned-x-ebs.html</guid>
        
        
      </item>
    
      <item>
        <title>How to add a second Mongoid 3 session in TDDIUM</title>
        <description>Folks who like Heroku and their addons certainly came across tddium, who promises to keep the tedium out of TDD ...lol.. &lt;br /&gt;&lt;br /&gt;Well, we just upgraded to Mongoid 3 and one of their usefull features are sessions. So now we can configure all kinds of sessions for all kinds of reasons:&lt;br /&gt;&lt;br /&gt;We might want to enforce consistency on some features. -- just use a session with consistency: :strong on its configuration.&lt;br /&gt;&lt;br /&gt;Or we might want our data to be eventually consistent, and have a higher troughput - so we use a consistency: :eventual.&lt;br /&gt;&lt;br /&gt;Anyway, this post is not about that, but about how to configure tddium to use multiple sessions: &lt;br /&gt;I took parts of this post &lt;a href=&quot;http://blog.nistu.de/2012/02/08/setting-up-tddium-for-a-rails-project&quot;&gt;http://blog.nistu.de/2012/02/08/setting-up-tddium-for-a-rails-project&lt;/a&gt; and with a little help from tddium suport, came up with this: &lt;br /&gt;&lt;div class=&quot;gistLoad&quot; data-id=&quot;3259158&quot; id=&quot;gist-3259158&quot;&gt;Loading ....&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;It worked for most of our tests.&amp;nbsp; We&#39;ll try and figure it out on Monday the ones that didn&#39;t work.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;** thanks for Moski for his post about posting gist into blogger&#39;s dynamic views. &lt;a href=&quot;http://http://blog.moski.me/2012/01/gist-with-bloggers-dynamic-views.html&quot;&gt;http://blog.moski.me/2012/01/gist-with-bloggers-dynamic-views.html&lt;/a&gt;   &lt;script src=&quot;https://raw.github.com/moski/gist-Blogger/master/public/gistLoader.js&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;</description>
        <pubDate>Sat, 04 Aug 2012 15:37:00 -0300</pubDate>
        <link>http://imiagao.github.io/2012/08/04/how-to-add-second-mongoid-3-session-in.html</link>
        <guid isPermaLink="true">http://imiagao.github.io/2012/08/04/how-to-add-second-mongoid-3-session-in.html</guid>
        
        
      </item>
    
      <item>
        <title>New IOP Provisioned EBS Volumes -- with a live MongoDB</title>
        <description>This weekend I´ll put the new iop provisioned EBS volumes for a spin ( or not, since they must be SSD´s backed devices ).&lt;br /&gt;&lt;br /&gt;Well, anyway, the server is ready. MMS-agent and munin-node are on place and waiting. All I have to do now is a rSync.&lt;br /&gt;&lt;br /&gt;I attached 10 x iop provisioned EBS volumes --1000 iops and distributed them as &amp;nbsp;follows:&lt;br /&gt;&lt;br /&gt;/dev/xvda - &amp;nbsp; &amp;nbsp; /&lt;br /&gt;/dev/md0 - /var/lib/mongodb&lt;br /&gt;/dev/xvdg &amp;nbsp; &amp;nbsp;- /var/lib/mongdb/journal&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;/dev/md0 is an 8 devices software RAID10 array&lt;br /&gt;&lt;br /&gt;Every one of them has 1000 iops provisioned. &lt;br /&gt;&lt;br /&gt;I would prefer to use this new EBS backed MongoDB over the instance store SSDs, but there´s a catch, there´s always a catch: As of now, one user can only have 10 of them, or being strict: you can only have 10,000 of them ( iops ).&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;http://1.bp.blogspot.com/-Igf8xNCh2fU/UBvn_rlkHqI/AAAAAAAAApE/KU1LjJsBqfk/s1600/Screen+Shot+2012-08-03+at+11.45.07+AM.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;147&quot; src=&quot;http://1.bp.blogspot.com/-Igf8xNCh2fU/UBvn_rlkHqI/AAAAAAAAApE/KU1LjJsBqfk/s320/Screen+Shot+2012-08-03+at+11.45.07+AM.png&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;So, let´s see how it goes. I´ll publish the results on Monday.&lt;br /&gt;&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;</description>
        <pubDate>Fri, 03 Aug 2012 12:04:00 -0300</pubDate>
        <link>http://imiagao.github.io/2012/08/03/new-iop-provisioned-ebs-volumes-with.html</link>
        <guid isPermaLink="true">http://imiagao.github.io/2012/08/03/new-iop-provisioned-ebs-volumes-with.html</guid>
        
        
      </item>
    
      <item>
        <title>MongoDB on Amazon ec2 with SSD</title>
        <description>We are using the new Amazon ec2 (h1.4xlarge) SSD boosted Machines to test and maybe production this week.&lt;br /&gt;&lt;span style=&quot;background-color: white;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;background-color: white;&quot;&gt;The copy velocity and the full resync is amazing. Renaming fields went a lot faster, but not as much as we´d like.&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;M&lt;span style=&quot;background-color: white;&quot;&gt;ap-reduce of about 20 million documents took less than 10 minutes.&amp;nbsp;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;Renaming a field on 10 Million documents took 20 minutes. Renaming on 320 Million, however, took more than 19 hours....&lt;br /&gt;&lt;br /&gt;Indexing is still a problem because mongodb uses only one core to do everything and can take as long as a common machine with attached EBS.&lt;br /&gt;&lt;br /&gt;That said, in real life situations the %lock on the DB goes down to half of the normal and the background flush avg is less than 200 ms. &amp;nbsp;I think that on the lung run it will be great once the paging starts to push the limits on our RAM.&lt;br /&gt;&lt;br /&gt;&lt;strike&gt;I´m eager to put it into production to see how it goes.&lt;/strike&gt; They are already in production since last week.&lt;br /&gt;Our app have never been faster, the paging are about 50/sec, but are almost unnoticed. &amp;nbsp;We are expecting to reach 10 MM impressions this Sunday!&lt;br /&gt;&lt;br /&gt;A few gotchas: we cannot save them as part of an AMI, so everytime you have to rebuild it, you have to make it with a full resync. And as of today Amazon only allows you to have two of those babies, so if you want more, you have to ask them...&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;Here´s how it compares in terms of %lock:&lt;br /&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;http://4.bp.blogspot.com/-kvRpRWecmHM/UBvg4_ftnZI/AAAAAAAAAos/JnKLOQ_5IeA/s1600/Screen+Shot+2012-08-03+at+11.32.46+AM.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;273&quot; src=&quot;http://4.bp.blogspot.com/-kvRpRWecmHM/UBvg4_ftnZI/AAAAAAAAAos/JnKLOQ_5IeA/s640/Screen+Shot+2012-08-03+at+11.32.46+AM.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;And here´s the SWEET IOP measures at the primary:&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;http://3.bp.blogspot.com/-zemf0oX3FV8/UBvhZe2scrI/AAAAAAAAAo0/U2gJOz5qCek/s1600/Screen+Shot+2012-08-03+at+11.35.27+AM.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;203&quot; src=&quot;http://3.bp.blogspot.com/-zemf0oX3FV8/UBvhZe2scrI/AAAAAAAAAo0/U2gJOz5qCek/s320/Screen+Shot+2012-08-03+at+11.35.27+AM.png&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;This weekend, I´ll put the newer iop provisioned EBS (http://aws.typepad.com/aws/2012/08/fast-forward-provisioned-iops-ebs.html) &amp;nbsp;to work on a secondary, as the older ones are suffering at 50% lock and they could get into a state where they will not be able to catch up the primary.&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;</description>
        <pubDate>Fri, 27 Jul 2012 15:50:00 -0300</pubDate>
        <link>http://imiagao.github.io/2012/07/27/mongodb-on-amazon-ec2-with-ssd.html</link>
        <guid isPermaLink="true">http://imiagao.github.io/2012/07/27/mongodb-on-amazon-ec2-with-ssd.html</guid>
        
        
      </item>
    
  </channel>
</rss>
